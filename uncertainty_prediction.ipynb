{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 22>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Adam\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorboardX\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SummaryWriter\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m metric_report, t2n, get_n_params\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BertConfig\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpredictive_models\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GBERT_Predict\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "import dill\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.optim import Adam\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from utils import metric_report, t2n, get_n_params\n",
    "from config import BertConfig\n",
    "from predictive_models import GBERT_Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Voc(object):\n",
    "    def __init__(self):\n",
    "        self.idx2word = {}\n",
    "        self.word2idx = {}\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            if word not in self.word2idx:\n",
    "                self.idx2word[len(self.word2idx)] = word\n",
    "                self.word2idx[word] = len(self.word2idx)\n",
    "\n",
    "\n",
    "class EHRTokenizer(object):\n",
    "    \"\"\"Runs end-to-end tokenization\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, special_tokens=(\"[PAD]\", \"[CLS]\", \"[MASK]\")):\n",
    "\n",
    "        self.vocab = Voc()\n",
    "\n",
    "        # special tokens\n",
    "        self.vocab.add_sentence(special_tokens)\n",
    "\n",
    "        self.rx_voc = self.add_vocab(os.path.join(data_dir, 'rx-vocab.txt'))\n",
    "        self.dx_voc = self.add_vocab(os.path.join(data_dir, 'dx-vocab.txt'))\n",
    "\n",
    "        # code only in multi-visit data\n",
    "        self.rx_voc_multi = Voc()\n",
    "        self.dx_voc_multi = Voc()\n",
    "        with open(os.path.join(data_dir, 'rx-vocab-multi.txt'), 'r') as fin:\n",
    "            for code in fin:\n",
    "                self.rx_voc_multi.add_sentence([code.rstrip('\\n')])\n",
    "        with open(os.path.join(data_dir, 'dx-vocab-multi.txt'), 'r') as fin:\n",
    "            for code in fin:\n",
    "                self.dx_voc_multi.add_sentence([code.rstrip('\\n')])\n",
    "\n",
    "    def add_vocab(self, vocab_file):\n",
    "        voc = self.vocab\n",
    "        specific_voc = Voc()\n",
    "        with open(vocab_file, 'r') as fin:\n",
    "            for code in fin:\n",
    "                voc.add_sentence([code.rstrip('\\n')])\n",
    "                specific_voc.add_sentence([code.rstrip('\\n')])\n",
    "        return specific_voc\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self.vocab.word2idx[token])\n",
    "        return ids\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        \"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"\n",
    "        tokens = []\n",
    "        for i in ids:\n",
    "            tokens.append(self.vocab.idx2word[i])\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class EHRDataset(Dataset):\n",
    "    def __init__(self, data_pd, tokenizer: EHRTokenizer, max_seq_len):\n",
    "        self.data_pd = data_pd\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = max_seq_len\n",
    "\n",
    "        self.sample_counter = 0\n",
    "\n",
    "        def transform_data(data):\n",
    "            \"\"\"\n",
    "            :param data: raw data form\n",
    "            :return: {subject_id, [adm, 2, codes]},\n",
    "            \"\"\"\n",
    "            records = {}\n",
    "            for subject_id in data['SUBJECT_ID'].unique():\n",
    "                item_df = data[data['SUBJECT_ID'] == subject_id]\n",
    "                patient = []\n",
    "                for _, row in item_df.iterrows():\n",
    "                    admission = [list(row['ICD9_CODE']), list(row['ATC4'])]\n",
    "                    patient.append(admission)\n",
    "                if len(patient) < 2:\n",
    "                    continue\n",
    "                records[subject_id] = patient\n",
    "            return records\n",
    "\n",
    "        self.records = transform_data(data_pd)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        cur_id = self.sample_counter\n",
    "        self.sample_counter += 1\n",
    "        subject_id = list(self.records.keys())[item]\n",
    "\n",
    "        def fill_to_max(l, seq):\n",
    "            while len(l) < seq:\n",
    "                l.append('[PAD]')\n",
    "            return l\n",
    "\n",
    "        \"\"\"extract input and output tokens\n",
    "        \"\"\"\n",
    "        input_tokens = []  # (2*max_len*adm)\n",
    "        output_dx_tokens = []  # (adm-1, l)\n",
    "        output_rx_tokens = []  # (adm-1, l)\n",
    "\n",
    "        for idx, adm in enumerate(self.records[subject_id]):\n",
    "            input_tokens.extend(\n",
    "                ['[CLS]'] + fill_to_max(list(adm[0]), self.seq_len - 1))\n",
    "            input_tokens.extend(\n",
    "                ['[CLS]'] + fill_to_max(list(adm[1]), self.seq_len - 1))\n",
    "            # output_rx_tokens.append(list(adm[1]))\n",
    "\n",
    "            if idx != 0:\n",
    "                output_rx_tokens.append(list(adm[1]))\n",
    "                output_dx_tokens.append(list(adm[0]))\n",
    "\n",
    "        \"\"\"convert tokens to id\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "        output_dx_labels = []  # (adm-1, dx_voc_size)\n",
    "        output_rx_labels = []  # (adm-1, rx_voc_size)\n",
    "\n",
    "        dx_voc_size = len(self.tokenizer.dx_voc_multi.word2idx)\n",
    "        rx_voc_size = len(self.tokenizer.rx_voc_multi.word2idx)\n",
    "        for tokens in output_dx_tokens:\n",
    "            tmp_labels = np.zeros(dx_voc_size)\n",
    "            tmp_labels[list(\n",
    "                map(lambda x: self.tokenizer.dx_voc_multi.word2idx[x], tokens))] = 1\n",
    "            output_dx_labels.append(tmp_labels)\n",
    "\n",
    "        for tokens in output_rx_tokens:\n",
    "            tmp_labels = np.zeros(rx_voc_size)\n",
    "            tmp_labels[list(\n",
    "                map(lambda x: self.tokenizer.rx_voc_multi.word2idx[x], tokens))] = 1\n",
    "            output_rx_labels.append(tmp_labels)\n",
    "\n",
    "        if cur_id < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"subject_id: %s\" % subject_id)\n",
    "            logger.info(\"input tokens: %s\" % \" \".join(\n",
    "                [str(x) for x in input_tokens]))\n",
    "            logger.info(\"input_ids: %s\" %\n",
    "                        \" \".join([str(x) for x in input_ids]))\n",
    "\n",
    "        assert len(input_ids) == (self.seq_len *\n",
    "                                  2 * len(self.records[subject_id]))\n",
    "        assert len(output_dx_labels) == (len(self.records[subject_id]) - 1)\n",
    "        # assert len(output_rx_labels) == len(self.records[subject_id])-1\n",
    "\n",
    "        cur_tensors = (torch.tensor(input_ids).view(-1, self.seq_len),\n",
    "                       torch.tensor(output_dx_labels, dtype=torch.float),\n",
    "                       torch.tensor(output_rx_labels, dtype=torch.float))\n",
    "\n",
    "        return cur_tensors\n",
    "\n",
    "\n",
    "def load_dataset(args):\n",
    "    data_dir = args.data_dir\n",
    "    max_seq_len = args.max_seq_length\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = EHRTokenizer(data_dir)\n",
    "\n",
    "    # load data\n",
    "    data = pd.read_pickle(os.path.join(data_dir, 'data-multi-visit.pkl'))\n",
    "\n",
    "    # load trian, eval, test data\n",
    "    ids_file = [os.path.join(data_dir, 'train-id.txt'),\n",
    "                os.path.join(data_dir, 'eval-id.txt'),\n",
    "                os.path.join(data_dir, 'test-id.txt')]\n",
    "\n",
    "    def load_ids(data, file_name):\n",
    "        \"\"\"\n",
    "        :param data: multi-visit data\n",
    "        :param file_name:\n",
    "        :return: raw data form\n",
    "        \"\"\"\n",
    "        ids = []\n",
    "        with open(file_name, 'r') as f:\n",
    "            for line in f:\n",
    "                ids.append(int(line.rstrip('\\n')))\n",
    "        return data[data['SUBJECT_ID'].isin(ids)].reset_index(drop=True)\n",
    "\n",
    "    return tokenizer, tuple(map(lambda x: EHRDataset(load_ids(data, x), tokenizer, max_seq_len), ids_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43margs\u001B[49m\u001B[38;5;241m.\u001B[39mdata_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124md\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "args.data_dir = 'd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.data_dir = 'd'\n",
    "\n",
    "tokenizer, (train_dataset, eval_dataset, test_dataset) = load_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, (train_dataset, eval_dataset, test_dataset) = load_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GBERT_Predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/user/iibi/nale/iiai-projects/G-Bert-remote/saved/GBert-baseline1_seed1203\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 5\u001B[0m model1 \u001B[38;5;241m=\u001B[39m \u001B[43mGBERT_Predict\u001B[49m\u001B[38;5;241m.\u001B[39mfrom_pretrained(path, tokenizer\u001B[38;5;241m=\u001B[39mtokenizer, device\u001B[38;5;241m=\u001B[39mdevice)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'GBERT_Predict' is not defined"
     ]
    }
   ],
   "source": [
    "path = '/user/iibi/nale/iiai-projects/G-Bert-remote/saved/GBert-baseline1_seed1203'\n",
    "\n",
    "\n",
    "\n",
    "model1 = GBERT_Predict.from_pretrained(path, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
